{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Reading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>task1</th>\n      <th>task2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...</td>\n      <td>HOF</td>\n      <td>PRFN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RT @airjunebug: When you're from the Bay but y...</td>\n      <td>HOF</td>\n      <td>PRFN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n      <td>NOT</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RT @SheLoveTimothy: He ainâ€™t on drugs he just ...</td>\n      <td>NOT</td>\n      <td>PRFN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...</td>\n      <td>NOT</td>\n      <td>NONE</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                text task1 task2\n0  hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...   HOF  PRFN\n1  RT @airjunebug: When you're from the Bay but y...   HOF  PRFN\n2  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   NOT  NONE\n3  RT @SheLoveTimothy: He ainâ€™t on drugs he just ...   NOT  PRFN\n4  RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...   NOT  NONE"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"hasoc.csv\")\n",
    "\n",
    "df = data[['text', 'task1', 'task2']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 . Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...\n1    rt @airjunebug: when you're from the bay but y...\n2    rt @donaldjtrumpjr: dear democrats: the americ...\n3    rt @shelovetimothy: he ainâ€™t on drugs he just ...\n4    rt @tavianjordan: summer â€˜19 iâ€™m coming for yo...\nName: clean_msg, dtype: object"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_msg'] = df['text'].apply(lambda x: x.lower())\n",
    "df['clean_msg'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Removal of Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": "0    hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚ ...\n1    rt airjunebug when youre from the bay but your...\n2    rt donaldjtrumpjr dear democrats the american ...\n3    rt shelovetimothy he ainâ€™t on drugs he just bo...\n4    rt tavianjordan summer â€˜19 iâ€™m coming for you ...\nName: clean_msg, dtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#library that contains punctuation\n",
    "import string\n",
    "# list of all punctuations we have\n",
    "print(string.punctuation)\n",
    " \n",
    "#defining the function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    " \n",
    "#storing the punctuation free text for both training and testing data\n",
    "df['clean_msg'] = df['clean_msg'].apply(lambda x:remove_punctuation(x))\n",
    "df['clean_msg'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Removal of Stopwords in Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    [hate, wen, females, hit, ah, nigga, with, tht...\n1    [rt, airjunebug, when, youre, from, the, bay, ...\n2    [rt, donaldjtrumpjr, dear, democrats, the, ame...\n3    [rt, shelovetimothy, he, ainâ€™t, on, drugs, he,...\n4    [rt, tavianjordan, summer, â€˜19, iâ€™m, coming, f...\nName: tokenised_clean_msg, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining function for tokenization\n",
    "import re\n",
    "#whitespace tokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "def tokenization(text):\n",
    "    return tk.tokenize(text)\n",
    " \n",
    "#applying function to the column for making tokens in both Training and Testing data\n",
    "df['tokenised_clean_msg']= df['clean_msg'].apply(lambda x: tokenization(x))\n",
    "df['tokenised_clean_msg'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\praka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "0    [hate, wen, females, hit, ah, nigga, tht, bro,...\n1    [rt, airjunebug, youre, bay, youre, really, ny...\n2    [rt, donaldjtrumpjr, dear, democrats, american...\n3    [rt, shelovetimothy, ainâ€™t, drugs, bored, shit...\n4    [rt, tavianjordan, summer, â€˜19, iâ€™m, coming, b...\nName: cleaned_tokens, dtype: object"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing nlp library\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    " \n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    " \n",
    "#applying the function for removal of stopwords\n",
    "df['cleaned_tokens']= df['tokenised_clean_msg'].apply(lambda x:remove_stopwords(x))\n",
    "df['cleaned_tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text[gpl]==0.4.0 in c:\\users\\praka\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\praka\\anaconda3\\lib\\site-packages (from clean-text[gpl]==0.4.0) (6.1.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\praka\\anaconda3\\lib\\site-packages (from clean-text[gpl]==0.4.0) (1.7.0)\n",
      "Requirement already satisfied: unidecode<2.0.0,>=1.1.1 in c:\\users\\praka\\anaconda3\\lib\\site-packages (from clean-text[gpl]==0.4.0) (1.3.6)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\praka\\anaconda3\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text[gpl]==0.4.0) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install clean-text[gpl]==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji==1.7 in c:\\users\\praka\\anaconda3\\lib\\site-packages (1.7.0)\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "! pip install emoji==1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cleaned_tokens'] = df['cleaned_tokens'].str.replace('[^A-Za-z0-9]', '', flags=re.UNICODE)\n",
    "from cleantext import clean\n",
    "\n",
    "def rem_em(text):\n",
    "    clean_txt = clean(text, no_emoji=True)\n",
    "    return clean_txt\n",
    "\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: rem_em(x))\n",
    "df['cleaned_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>tokenised_clean_msg</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚ ...</td>\n",
       "      <td>[hate, wen, females, hit, ah, nigga, with, tht...</td>\n",
       "      <td>[hate, wen, females, hit, ah, nigga, tht, bro,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>rt airjunebug when youre from the bay but your...</td>\n",
       "      <td>[rt, airjunebug, when, youre, from, the, bay, ...</td>\n",
       "      <td>[rt, airjunebug, youre, bay, youre, really, ny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>rt donaldjtrumpjr dear democrats the american ...</td>\n",
       "      <td>[rt, donaldjtrumpjr, dear, democrats, the, ame...</td>\n",
       "      <td>[rt, donaldjtrumpjr, dear, democrats, american...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SheLoveTimothy: He ainâ€™t on drugs he just ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>rt shelovetimothy he ainâ€™t on drugs he just bo...</td>\n",
       "      <td>[rt, shelovetimothy, he, ainâ€™t, on, drugs, he,...</td>\n",
       "      <td>[rt, shelovetimothy, ainâ€™t, drugs, bored, shit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>rt tavianjordan summer â€˜19 iâ€™m coming for you ...</td>\n",
       "      <td>[rt, tavianjordan, summer, â€˜19, iâ€™m, coming, f...</td>\n",
       "      <td>[rt, tavianjordan, summer, â€˜19, iâ€™m, coming, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>RT @FilthyArt_: TONIGHT TONIGHT TONIGHT \\n\\nCa...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>rt filthyart tonight tonight tonight \\n\\ncatch...</td>\n",
       "      <td>[rt, filthyart, tonight, tonight, tonight, cat...</td>\n",
       "      <td>[rt, filthyart, tonight, tonight, tonight, cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3790</th>\n",
       "      <td>RT @abbn0rmal_: Eat my ass</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>rt abbn0rmal eat my ass</td>\n",
       "      <td>[rt, abbn0rmal, eat, my, ass]</td>\n",
       "      <td>[rt, abbn0rmal, eat, ass]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>RT @FlyTPA: BREAKING NEWS: TPA is about to get...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>rt flytpa breaking news tpa is about to get ev...</td>\n",
       "      <td>[rt, flytpa, breaking, news, tpa, is, about, t...</td>\n",
       "      <td>[rt, flytpa, breaking, news, tpa, get, even, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>RT @StarrThaRapper: Itâ€™s been a hr FUCK THAT G...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>rt starrtharapper itâ€™s been a hr fuck that gam...</td>\n",
       "      <td>[rt, starrtharapper, itâ€™s, been, a, hr, fuck, ...</td>\n",
       "      <td>[rt, starrtharapper, itâ€™s, hr, fuck, game, ðŸ‘¿ðŸ‘¿,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>You don't know who iam but i know youðŸ˜„</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>you dont know who iam but i know youðŸ˜„</td>\n",
       "      <td>[you, dont, know, who, iam, but, i, know, youðŸ˜„]</td>\n",
       "      <td>[dont, know, iam, know, youðŸ˜„]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3794 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text task1 task2  \\\n",
       "0     hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...   HOF  PRFN   \n",
       "1     RT @airjunebug: When you're from the Bay but y...   HOF  PRFN   \n",
       "2     RT @DonaldJTrumpJr: Dear Democrats: The Americ...   NOT  NONE   \n",
       "3     RT @SheLoveTimothy: He ainâ€™t on drugs he just ...   NOT  PRFN   \n",
       "4     RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...   NOT  NONE   \n",
       "...                                                 ...   ...   ...   \n",
       "3789  RT @FilthyArt_: TONIGHT TONIGHT TONIGHT \\n\\nCa...   NOT  NONE   \n",
       "3790                         RT @abbn0rmal_: Eat my ass   HOF  PRFN   \n",
       "3791  RT @FlyTPA: BREAKING NEWS: TPA is about to get...   NOT  NONE   \n",
       "3792  RT @StarrThaRapper: Itâ€™s been a hr FUCK THAT G...   HOF  PRFN   \n",
       "3793             You don't know who iam but i know youðŸ˜„   NOT  NONE   \n",
       "\n",
       "                                              clean_msg  \\\n",
       "0     hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚ ...   \n",
       "1     rt airjunebug when youre from the bay but your...   \n",
       "2     rt donaldjtrumpjr dear democrats the american ...   \n",
       "3     rt shelovetimothy he ainâ€™t on drugs he just bo...   \n",
       "4     rt tavianjordan summer â€˜19 iâ€™m coming for you ...   \n",
       "...                                                 ...   \n",
       "3789  rt filthyart tonight tonight tonight \\n\\ncatch...   \n",
       "3790                            rt abbn0rmal eat my ass   \n",
       "3791  rt flytpa breaking news tpa is about to get ev...   \n",
       "3792  rt starrtharapper itâ€™s been a hr fuck that gam...   \n",
       "3793              you dont know who iam but i know youðŸ˜„   \n",
       "\n",
       "                                    tokenised_clean_msg  \\\n",
       "0     [hate, wen, females, hit, ah, nigga, with, tht...   \n",
       "1     [rt, airjunebug, when, youre, from, the, bay, ...   \n",
       "2     [rt, donaldjtrumpjr, dear, democrats, the, ame...   \n",
       "3     [rt, shelovetimothy, he, ainâ€™t, on, drugs, he,...   \n",
       "4     [rt, tavianjordan, summer, â€˜19, iâ€™m, coming, f...   \n",
       "...                                                 ...   \n",
       "3789  [rt, filthyart, tonight, tonight, tonight, cat...   \n",
       "3790                      [rt, abbn0rmal, eat, my, ass]   \n",
       "3791  [rt, flytpa, breaking, news, tpa, is, about, t...   \n",
       "3792  [rt, starrtharapper, itâ€™s, been, a, hr, fuck, ...   \n",
       "3793    [you, dont, know, who, iam, but, i, know, youðŸ˜„]   \n",
       "\n",
       "                                         cleaned_tokens  \n",
       "0     [hate, wen, females, hit, ah, nigga, tht, bro,...  \n",
       "1     [rt, airjunebug, youre, bay, youre, really, ny...  \n",
       "2     [rt, donaldjtrumpjr, dear, democrats, american...  \n",
       "3     [rt, shelovetimothy, ainâ€™t, drugs, bored, shit...  \n",
       "4     [rt, tavianjordan, summer, â€˜19, iâ€™m, coming, b...  \n",
       "...                                                 ...  \n",
       "3789  [rt, filthyart, tonight, tonight, tonight, cat...  \n",
       "3790                          [rt, abbn0rmal, eat, ass]  \n",
       "3791  [rt, flytpa, breaking, news, tpa, get, even, b...  \n",
       "3792  [rt, starrtharapper, itâ€™s, hr, fuck, game, ðŸ‘¿ðŸ‘¿,...  \n",
       "3793                      [dont, know, iam, know, youðŸ˜„]  \n",
       "\n",
       "[3794 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df[['cleaned_tokens', 'task1', 'task2']]\n",
    "final_df.to_csv(\"final_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['cleaned_tokens'].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df['task1'].values\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ham label as 0 and spam as 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['msg_type']= df['task1'].map({'HOF': 0, 'NOT': 1})\n",
    "msg_label = df['msg_type']\n",
    "# Split data into train and test\n",
    "train_msg, test_msg, train_labels, test_labels = train_test_split(df['cleaned_tokens'], msg_label, test_size=0.2, random_state=434)\n",
    "msg_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters\n",
    "max_len = max([len(s.split()) for s in train_msg])\n",
    "print(\"\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 500)\n",
    "tokenizer.fit_on_texts(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word_index \n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many words \n",
    "tot_words = len(word_index)\n",
    "print('There are %s unique tokens in training data. ' % tot_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Sequencing and padding on training and testing \n",
    "training_sequences = tokenizer.texts_to_sequences(train_msg)\n",
    "training_padded = pad_sequences (training_sequences, maxlen = max_len, padding = 'post')\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_msg)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_len,padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of train tensor\n",
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of testing tensor: ', testing_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before padding\n",
    "len(training_sequences[0]), len(training_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After padding\n",
    "len(training_padded[0]), len(training_padded[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = 8296  # As defined earlier\n",
    "embeding_dim = 16\n",
    "drop_value = 0.2 # dropout\n",
    "n_dense = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense model architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D, Conv1D, MaxPooling1D, Activation,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeding_dim, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dropout(drop_value))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam' ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a dense spam detector model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "num_epochs = 30\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(training_padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels),callbacks =[early_stop], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance on test data \n",
    "model.evaluate(testing_padded, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as a dataframe \n",
    "import matplotlib.pyplot as plt\n",
    "metrics = pd.DataFrame(history.history)\n",
    "# Rename column\n",
    "metrics.rename(columns = {'loss': 'Training_Loss', 'accuracy': 'Training_Accuracy', 'val_loss': 'Validation_Loss', 'val_accuracy': 'Validation_Accuracy'}, inplace = True)\n",
    "def plot_graphs1(var1, var2, string):\n",
    "    metrics[[var1, var2]].plot()\n",
    "    plt.title('Training and Validation ' + string)\n",
    "    plt.xlabel ('Number of epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([var1, var2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs1('Training_Loss', 'Validation_Loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs1('Training_Accuracy', 'Validation_Accuracy', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = model.predict(testing_padded)\n",
    "prediction = []\n",
    "for predict in temp:\n",
    "    if predict < 0.5:\n",
    "        prediction.append(0)\n",
    "    else:\n",
    "        prediction.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(prediction, test_labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = testing_padded[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample = np.expand_dims(sample, axis=0)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(sample)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hate wen females hit ah nigga with tht bro Im tryna make u my la sweety fuck ah bro\"\n",
    "\n",
    "def prepros(sample):\n",
    "    sample = sample.lower()\n",
    "    print(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    print(sample)\n",
    "    sample = tokenization(sample)\n",
    "    print(sample)\n",
    "    sample = remove_stopwords(sample)\n",
    "    print(sample)\n",
    "    sample = rem_em(sample)\n",
    "    print(sample)\n",
    "    return sample\n",
    "\n",
    "sample = prepros(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequences = tokenizer.texts_to_sequences([sample])\n",
    "print(sample_sequences)\n",
    "\n",
    "sample_padded = pad_sequences (sample_sequences, maxlen = max_len, padding = 'post', truncating = 'post' )\n",
    "print(sample_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(sample_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = \"RT @TavianJordan: Summer '19 I'm coming for you ! No boring shit ! Beach days, road trips, kickbacks and HOT DAYS ! I'm ready I'm ready I'mâ€¦\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = prepros(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequences = tokenizer.texts_to_sequences([sample1])\n",
    "print(sample_sequences)\n",
    "\n",
    "sample_padded = pad_sequences (sample_sequences, maxlen = max_len, padding = 'post', truncating = 'post' )\n",
    "print(sample_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(sample_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['cleaned_tokens'].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = data['task2'].unique()\n",
    "Class.sort()\n",
    "print(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = data.iloc[:,-1].values\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder() \n",
    "y2 = encoder.fit_transform(np.array(y2).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ham label as 0 and spam as 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_msg, test_msg, train_labels, test_labels = train_test_split(df['cleaned_tokens'], y2, test_size=0.2, random_state=434, stratify=y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense model architecture\n",
    "#Dense model architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D, Conv1D, MaxPooling1D, Activation,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeding_dim, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dropout(drop_value))\n",
    "model.add(Dense(4, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam' ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a dense spam detector model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "num_epochs = 50\n",
    "history = model.fit(training_padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as a dataframe \n",
    "import matplotlib.pyplot as plt\n",
    "metrics = pd.DataFrame(history.history)\n",
    "# Rename column\n",
    "metrics.rename(columns = {'loss': 'Training_Loss', 'accuracy': 'Training_Accuracy', 'val_loss': 'Validation_Loss', 'val_accuracy': 'Validation_Accuracy'}, inplace = True)\n",
    "def plot_graphs1(var1, var2, string):\n",
    "    metrics[[var1, var2]].plot()\n",
    "    plt.title('Training and Validation ' + string)\n",
    "    plt.xlabel ('Number of epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([var1, var2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs1('Training_Loss', 'Validation_Loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs1('Training_Accuracy', 'Validation_Accuracy', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = \"@brewer383 @d_heard00 @daishatatianna I DONT NEED YOURE WHOLE LIFE STORY WHEN YOUR SHIT IS ON PUBLIC RECORD ðŸ˜‚ðŸ˜‚ðŸ˜‚ have a look yourself\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = prepros(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequences = tokenizer.texts_to_sequences([sample1])\n",
    "print(sample_sequences)\n",
    "\n",
    "sample_padded = pad_sequences (sample_sequences, maxlen = max_len, padding = 'post', truncating = 'post' )\n",
    "print(sample_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(sample_padded, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = \"@Boogie2988 I'm assuming you are talking about GOT but I don't watch that show so I have no idea what you're talking about.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = prepros(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequences = tokenizer.texts_to_sequences([sample2])\n",
    "print(sample_sequences)\n",
    "\n",
    "sample_padded = pad_sequences (sample_sequences, maxlen = max_len, padding = 'post', truncating = 'post' )\n",
    "print(sample_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(sample_padded, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = \"RT @DonaldJTrumpJr: Dear Democrats: The American people arenâ€™t stupid, they know what spying is and no amount of gaslighting will change thâ€¦\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = prepros(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequences = tokenizer.texts_to_sequences([sample2])\n",
    "print(sample_sequences)\n",
    "\n",
    "sample_padded = pad_sequences (sample_sequences, maxlen = max_len, padding = 'post', truncating = 'post' )\n",
    "print(sample_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(sample_padded, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.argmax(result, axis=1)[0]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.4.0\n",
      "anyio==3.6.2\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.2.3\n",
      "asgiref==3.6.0\n",
      "astroid==2.12.12\n",
      "asttokens==2.2.1\n",
      "astunparse==1.6.3\n",
      "attrs==23.1.0\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.12.2\n",
      "bleach==6.0.0\n",
      "cachetools==5.3.0\n",
      "certifi==2023.5.7\n",
      "cffi==1.15.1\n",
      "charset-normalizer==3.1.0\n",
      "clean-text==0.4.0\n",
      "cleantext==1.1.4\n",
      "click==8.1.3\n",
      "colorama==0.4.6\n",
      "comm==0.1.3\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.6\n",
      "Django==4.2.1\n",
      "emoji==1.7.0\n",
      "executing==1.2.0\n",
      "fastjsonschema==2.17.1\n",
      "flatbuffers==23.5.9\n",
      "fqdn==1.5.1\n",
      "ftfy==6.1.1\n",
      "gast==0.4.0\n",
      "google-auth==2.18.1\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.54.2\n",
      "h5py==3.8.0\n",
      "idna==3.4\n",
      "importlib-metadata==6.6.0\n",
      "ipykernel==6.23.1\n",
      "ipython==8.13.2\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==8.0.6\n",
      "isoduration==20.11.0\n",
      "isort==5.10.1\n",
      "jax==0.4.10\n",
      "jedi==0.18.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "jsonpointer==2.3\n",
      "jsonschema==4.17.3\n",
      "jupyter==1.0.0\n",
      "jupyter-client==8.2.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-core==5.3.0\n",
      "jupyter-events==0.6.3\n",
      "jupyter-server==2.5.0\n",
      "jupyter-server-terminals==0.4.4\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab-widgets==3.0.7\n",
      "keras==2.12.0\n",
      "lazy-object-proxy==1.8.0\n",
      "libclang==16.0.0\n",
      "Markdown==3.4.3\n",
      "MarkupSafe==2.1.2\n",
      "matplotlib-inline==0.1.6\n",
      "mccabe==0.7.0\n",
      "mistune==2.0.5\n",
      "ml-dtypes==0.1.0\n",
      "nbclassic==1.0.0\n",
      "nbclient==0.8.0\n",
      "nbconvert==7.4.0\n",
      "nbformat==5.8.0\n",
      "nest-asyncio==1.5.6\n",
      "nltk==3.8.1\n",
      "notebook==6.5.4\n",
      "notebook-shim==0.2.3\n",
      "numpy==1.23.5\n",
      "oauthlib==3.2.2\n",
      "opt-einsum==3.3.0\n",
      "packaging==23.1\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "pickleshare==0.7.5\n",
      "platformdirs==2.5.4\n",
      "prometheus-client==0.16.0\n",
      "prompt-toolkit==3.0.38\n",
      "protobuf==4.23.1\n",
      "psutil==5.9.5\n",
      "pure-eval==0.2.2\n",
      "pyasn1==0.5.0\n",
      "pyasn1-modules==0.3.0\n",
      "pycparser==2.21\n",
      "Pygments==2.15.1\n",
      "pylint==2.15.5\n",
      "pyrsistent==0.19.3\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "pywin32==306\n",
      "pywinpty==2.0.10\n",
      "PyYAML==6.0\n",
      "pyzmq==25.0.2\n",
      "qtconsole==5.4.3\n",
      "QtPy==2.3.1\n",
      "regex==2023.5.5\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rsa==4.9\n",
      "scipy==1.10.1\n",
      "Send2Trash==1.8.2\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.4.1\n",
      "sqlparse==0.4.4\n",
      "stack-data==0.6.2\n",
      "tensorboard==2.12.3\n",
      "tensorboard-data-server==0.7.0\n",
      "tensorflow==2.12.0\n",
      "tensorflow-estimator==2.12.0\n",
      "tensorflow-intel==2.12.0\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "termcolor==2.3.0\n",
      "terminado==0.17.1\n",
      "tinycss2==1.2.1\n",
      "tomli==2.0.1\n",
      "tomlkit==0.11.6\n",
      "tornado==6.3.2\n",
      "tqdm==4.65.0\n",
      "traitlets==5.9.0\n",
      "typing-extensions==4.4.0\n",
      "tzdata==2023.3\n",
      "Unidecode==1.3.6\n",
      "uri-template==1.2.0\n",
      "urllib3==1.26.15\n",
      "wcwidth==0.2.6\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.5.2\n",
      "Werkzeug==2.3.4\n",
      "widgetsnbextension==4.0.7\n",
      "wrapt==1.14.1\n",
      "zipp==3.15.0\n"
     ]
    }
   ],
   "source": [
    "! pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cleantext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c2d64199872f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcleantext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cleantext'"
     ]
    }
   ],
   "source": [
    "import cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}